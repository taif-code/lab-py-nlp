{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "acf31a39",
      "metadata": {
        "id": "acf31a39"
      },
      "source": [
        "# **Comprehensive NLP Lab: From Preprocessing to Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb5fcd9",
      "metadata": {
        "id": "0bb5fcd9"
      },
      "source": [
        "In this lab, you will explore a wide range of Natural Language Processing (NLP) techniques, from basic text preprocessing to advanced feature extraction and analysis. By the end of this lab, you will be able to:\n",
        "\n",
        "1. **Tokenize** and preprocess text data.\n",
        "2. Remove **stop words** and **punctuation**.\n",
        "3. Apply **stemming** and **lemmatization**.\n",
        "4. Extract features using **Bag of Words (BoW)** and **TF-IDF**.\n",
        "5. Generate **n-grams** to capture contextual information.\n",
        "6. Evaluate the impact of different preprocessing techniques on text data.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bdb53e",
      "metadata": {
        "id": "f0bdb53e"
      },
      "source": [
        "## **1. Setup the Environment**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa12605",
      "metadata": {
        "id": "9aa12605"
      },
      "source": [
        "Before we begin, ensure you have the necessary libraries installed. Run the following cell to install them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6dd9b473",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd9b473",
        "outputId": "0f23ec10-2bfa-4540-aafe-0b168a9f0a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3839e6cf",
      "metadata": {
        "id": "3839e6cf"
      },
      "source": [
        "Now, import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e27aa77b",
      "metadata": {
        "id": "e27aa77b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c5a2544b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5a2544b",
        "outputId": "33ab9841-ff83-48e6-9aca-eb02d7f5bceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Download NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec67490",
      "metadata": {
        "id": "cec67490"
      },
      "source": [
        "## **2. Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6877c3",
      "metadata": {
        "id": "5b6877c3"
      },
      "source": [
        "### **Exercise 1: Tokenization and Stop Word Removal**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956bdaa1",
      "metadata": {
        "id": "956bdaa1"
      },
      "source": [
        "Tokenize the following text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d82fcea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d82fcea5",
        "outputId": "a8407c03-8c16-4446-ccf0-c2e456b40bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\n",
            "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'study', '!', 'It', 'involves', 'analyzing', 'and', 'understanding', 'human', 'language', '.']\n",
            "Filtered Tokens (After Stop Word Removal): ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Ensure stopwords are downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Get the list of stop words in English\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Filtered Tokens (After Stop Word Removal):\", filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26303ab9",
      "metadata": {
        "id": "26303ab9"
      },
      "source": [
        "Remove stop words and store the result in a variable called `filtered_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d09992c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d09992c8",
        "outputId": "8fd805c2-ee9e-4bcd-decb-3e5542d004ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary NLTK components\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Get the list of stop words in English\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words and punctuation from the tokens\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
        "\n",
        "# Display the filtered tokens\n",
        "print(filtered_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "14120510",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14120510",
        "outputId": "7b14db14-fe20-4b17-fc20-b52585ed9814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "print(\"Filtered Tokens:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f0b30d",
      "metadata": {
        "id": "50f0b30d"
      },
      "source": [
        "### **Exercise 2: Stemming and Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6949309",
      "metadata": {
        "id": "c6949309"
      },
      "source": [
        "Apply stemming and lemmatization to the `filtered_tokens`. Compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03dd22dd",
      "metadata": {
        "id": "03dd22dd"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d063d90e",
      "metadata": {
        "id": "d063d90e"
      },
      "source": [
        "Apply stemming and store the result in `stemmed_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6120c6d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6120c6d0",
        "outputId": "f6554523-aa48-4f73-9422-559aebaf3f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n",
            "Stemmed Tokens: ['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'studi', 'involv', 'analyz', 'understand', 'human', 'languag']\n",
            "Lemmatized Tokens: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "# Import necessary NLTK components\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Filtered tokens from the previous step\n",
        "filtered_tokens = ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n",
        "\n",
        "# Apply stemming to the filtered tokens\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# Apply lemmatization to the filtered tokens\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Tokens:\", filtered_tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c2c5d353",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2c5d353",
        "outputId": "c1541132-5ff7-4c8a-da4f-1d6fa3fcd8f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens: ['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'studi', 'involv', 'analyz', 'understand', 'human', 'languag']\n"
          ]
        }
      ],
      "source": [
        "print(\"Stemmed Tokens:\", stemmed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2dc333a",
      "metadata": {
        "id": "a2dc333a"
      },
      "source": [
        "Apply lemmatization and store the result in `lemmatized_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c2b23234",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2b23234",
        "outputId": "973416f5-e9c9-4516-c1fe-2da5b526766b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary NLTK components\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example filtered tokens from the previous step (assuming stop words removed)\n",
        "filtered_tokens = ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n",
        "\n",
        "# Apply lemmatization to the filtered tokens\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "\n",
        "# Display the lemmatized tokens\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "08f49cad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08f49cad",
        "outputId": "0d790d44-8ede-4201-8b35-4ea179ca9c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20531b9b",
      "metadata": {
        "id": "20531b9b"
      },
      "source": [
        "## **3. Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5fbd840",
      "metadata": {
        "id": "a5fbd840"
      },
      "source": [
        "### **Exercise 3: Bag of Words (BoW)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7765074",
      "metadata": {
        "id": "b7765074"
      },
      "source": [
        "Use the `CountVectorizer` from `scikit-learn` to create a Bag of Words representation of the following corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "39c86f37",
      "metadata": {
        "id": "39c86f37"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"I love NLP.\",\n",
        "    \"NLP is amazing.\",\n",
        "    \"I enjoy learning new things in NLP.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "45334917",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45334917",
        "outputId": "2fc03ec2-faca-45d3-bbf9-f190e431a00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Representation (Token Counts):\n",
            "[[0 0 0 0 0 1 0 1 0]\n",
            " [1 0 0 1 0 0 0 1 0]\n",
            " [0 1 1 0 1 0 1 1 1]]\n",
            "\n",
            "Feature Names (Words in the corpus):\n",
            "['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    \"I love NLP.\",\n",
        "    \"NLP is amazing.\",\n",
        "    \"I enjoy learning new things in NLP.\"\n",
        "]\n",
        "\n",
        "# Step 1: Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Step 2: Fit and transform the corpus into a BoW representation\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert the result into an array to view the token counts\n",
        "bow_representation = X.toarray()\n",
        "\n",
        "# Display the BoW representation (token counts)\n",
        "print(\"BoW Representation (Token Counts):\")\n",
        "print(bow_representation)\n",
        "\n",
        "# Display the feature names (words)\n",
        "print(\"\\nFeature Names (Words in the corpus):\")\n",
        "print(vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bfb3b95e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfb3b95e",
        "outputId": "566a5ee9-1304-4abd-8370-96693905e15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            " [[0 0 0 0 0 1 0 1 0]\n",
            " [1 0 0 1 0 0 0 1 0]\n",
            " [0 1 1 0 1 0 1 1 1]]\n",
            "Vocabulary: ['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
          ]
        }
      ],
      "source": [
        "print(\"Bag of Words:\\n\", X.toarray())\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d79220",
      "metadata": {
        "id": "b0d79220"
      },
      "source": [
        "### **Exercise 4: TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55622108",
      "metadata": {
        "id": "55622108"
      },
      "source": [
        "Use the `TfidfVectorizer` from `scikit-learn` to create a TF-IDF representation of the same corpus. Store the result in `X_tfidf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ba20ee5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba20ee5c",
        "outputId": "7aaae612-8c87-4549-b8c6-9e9371aebec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Representation:\n",
            "[[0.         0.         0.         0.         0.         0.861037\n",
            "  0.         0.50854232 0.        ]\n",
            " [0.65249088 0.         0.         0.65249088 0.         0.\n",
            "  0.         0.38537163 0.        ]\n",
            " [0.         0.43238509 0.43238509 0.         0.43238509 0.\n",
            "  0.43238509 0.2553736  0.43238509]]\n",
            "\n",
            "Feature Names (Words in the corpus):\n",
            "['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    \"I love NLP.\",\n",
        "    \"NLP is amazing.\",\n",
        "    \"I enjoy learning new things in NLP.\"\n",
        "]\n",
        "\n",
        "# Step 1: Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Step 2: Fit and transform the corpus into a TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert the result into an array to view the TF-IDF values\n",
        "tfidf_representation = X_tfidf.toarray()\n",
        "\n",
        "# Display the TF-IDF representation\n",
        "print(\"TF-IDF Representation:\")\n",
        "print(tfidf_representation)\n",
        "\n",
        "# Display the feature names (words in the corpus)\n",
        "print(\"\\nFeature Names (Words in the corpus):\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e0df3ff9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0df3ff9",
        "outputId": "27768c69-cea2-426f-d042-740012ad67c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF:\n",
            " [[0.         0.         0.         0.         0.         0.861037\n",
            "  0.         0.50854232 0.        ]\n",
            " [0.65249088 0.         0.         0.65249088 0.         0.\n",
            "  0.         0.38537163 0.        ]\n",
            " [0.         0.43238509 0.43238509 0.         0.43238509 0.\n",
            "  0.43238509 0.2553736  0.43238509]]\n",
            "Vocabulary: ['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a6cee3",
      "metadata": {
        "id": "a0a6cee3"
      },
      "source": [
        "### **Exercise 5: N-grams**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c9d66d",
      "metadata": {
        "id": "95c9d66d"
      },
      "source": [
        "Generate `bigrams (2-grams)` from the corpus using `CountVectorizer`. Store the result in `X_bigram`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9de7b987",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9de7b987",
        "outputId": "d5364f81-7b90-4226-d291-5805af2ab41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Representation (Token Counts):\n",
            "[[0 0 0 0 1 0 0 0]\n",
            " [0 0 1 0 0 0 1 0]\n",
            " [1 1 0 1 0 1 0 1]]\n",
            "\n",
            "Feature Names (Bigrams in the corpus):\n",
            "['enjoy learning' 'in nlp' 'is amazing' 'learning new' 'love nlp'\n",
            " 'new things' 'nlp is' 'things in']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    \"I love NLP.\",\n",
        "    \"NLP is amazing.\",\n",
        "    \"I enjoy learning new things in NLP.\"\n",
        "]\n",
        "\n",
        "# Step 1: Initialize the CountVectorizer with ngram_range=(2, 2) for bigrams\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Step 2: Fit and transform the corpus into a bigram representation\n",
        "X_bigram = bigram_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert the result into an array to view the bigram counts\n",
        "bigram_representation = X_bigram.toarray()\n",
        "\n",
        "# Display the bigram representation (counts of bigrams)\n",
        "print(\"Bigram Representation (Token Counts):\")\n",
        "print(bigram_representation)\n",
        "\n",
        "# Display the feature names (bigrams in the corpus)\n",
        "print(\"\\nFeature Names (Bigrams in the corpus):\")\n",
        "print(bigram_vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7b210fc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b210fc9",
        "outputId": "9aa8ced1-4322-4518-8d9f-4a508d2148c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams:\n",
            " [[0 0 0 0 1 0 0 0]\n",
            " [0 0 1 0 0 0 1 0]\n",
            " [1 1 0 1 0 1 0 1]]\n",
            "Bigram Vocabulary: ['enjoy learning' 'in nlp' 'is amazing' 'learning new' 'love nlp'\n",
            " 'new things' 'nlp is' 'things in']\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigrams:\\n\", X_bigram.toarray())\n",
        "print(\"Bigram Vocabulary:\", bigram_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7896acb",
      "metadata": {
        "id": "f7896acb"
      },
      "source": [
        "## **4. Advanced Exercise: Custom Preprocessing Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68072a1e",
      "metadata": {
        "id": "68072a1e"
      },
      "source": [
        "### **Exercise 6: Build a Custom Preprocessing Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb33268",
      "metadata": {
        "id": "7bb33268"
      },
      "source": [
        "Combine all the preprocessing steps (tokenization, stop word removal, punctuation removal, stemming/lemmatization) into a single function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cfff29b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfff29b9",
        "outputId": "c46da4ac-7da1-4802-e51f-817e7a6de47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and get stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define the custom preprocessing pipeline function\n",
        "def text_preprocessing_pipeline(text):\n",
        "    # Step 1: Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 2: Remove stop words\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    # Step 3: Remove punctuation (already handled with stopwords removal)\n",
        "    # Step 4: Apply lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply this function to the given text\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "processed_text = text_preprocessing_pipeline(text)\n",
        "\n",
        "# Display the processed text\n",
        "print(\"Processed Text:\", processed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8170a6c",
      "metadata": {
        "id": "b8170a6c"
      },
      "source": [
        "Apply this function to the following text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e1777799",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1777799",
        "outputId": "2666a6d2-21ac-453d-8861-4a32eb9ba339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "processed_text = text_preprocessing_pipeline(text)\n",
        "\n",
        "# Display the processed text\n",
        "print(\"Processed Text:\", processed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3625df20",
      "metadata": {
        "id": "3625df20"
      },
      "source": [
        "## **5. Evaluation of Preprocessing Techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a666da0",
      "metadata": {
        "id": "3a666da0"
      },
      "source": [
        "### **Exercise 7: Compare Preprocessing Techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae05a37",
      "metadata": {
        "id": "dae05a37"
      },
      "source": [
        "Compare the results of stemming and lemmatization on the following sentence. Store the results in `stemmed_tokens` and `lemmatized_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1e70cba9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e70cba9",
        "outputId": "de4a1d62-f817-4d81-e894-ca9c603fe95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['cats', 'playing', 'mice', 'garden']\n",
            "Stemmed Tokens: ['cat', 'play', 'mice', 'garden']\n",
            "Lemmatized Tokens: ['cat', 'playing', 'mouse', 'garden']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The cats are playing with the mice in the garden.\"\n",
        "\n",
        "# Step 1: Tokenize the sentence and remove stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(sentence)\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
        "\n",
        "# Step 2: Apply stemming\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# Step 3: Apply lemmatization\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Tokens:\", filtered_tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "125ca6a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "125ca6a2",
        "outputId": "4ea2a219-cd2b-4eb6-e49a-7b152e54adf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['cats', 'playing', 'mice', 'garden']\n",
            "Stemmed Tokens: ['cat', 'play', 'mice', 'garden']\n",
            "Lemmatized Tokens: ['cat', 'playing', 'mouse', 'garden']\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Tokens:\", filtered_tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32c465f",
      "metadata": {
        "id": "c32c465f"
      },
      "source": [
        "## **6. Real-World Dataset: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a598d9",
      "metadata": {
        "id": "c5a598d9"
      },
      "source": [
        "### **Exercise 8: Preprocess and Analyze Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d056ab80",
      "metadata": {
        "id": "d056ab80"
      },
      "source": [
        "In this exercise, you will work with a real-world dataset of tweets. The dataset contains 5000 positive and 5000 negative tweets. Your task is to preprocess the tweets and extract features for sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d1e37f72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1e37f72",
        "outputId": "103eb343-45c0-45f9-842d-1451383b3e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c2c60819",
      "metadata": {
        "id": "c2c60819"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "from nltk.corpus import twitter_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43847ae",
      "metadata": {
        "id": "c43847ae"
      },
      "source": [
        "Load the dataset of positive and negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5b423ebc",
      "metadata": {
        "id": "5b423ebc"
      },
      "outputs": [],
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339b8248",
      "metadata": {
        "id": "339b8248"
      },
      "source": [
        "Combine them into a single list called ``all_tweets`` and create a corresponding list of labels called `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "271a4ee0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271a4ee0",
        "outputId": "e4e5980b-23c8-4a41-caab-7c53b7a65295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 tweets: ['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', '@97sides CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']\n",
            "First 5 labels: [1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "# Load the positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Step 1: Combine the positive and negative tweets into a single list called all_tweets\n",
        "all_tweets = positive_tweets + negative_tweets\n",
        "\n",
        "# Step 2: Create a corresponding list of labels, where 1 is for positive tweets and 0 for negative tweets\n",
        "labels = [1] * len(positive_tweets) + [0] * len(negative_tweets)\n",
        "\n",
        "# Display the combined data and labels\n",
        "print(\"First 5 tweets:\", all_tweets[:5])\n",
        "print(\"First 5 labels:\", labels[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "32ec3ed8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ec3ed8",
        "outputId": "db7b7302-62a4-48f1-f0e2-084877e6bc91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Tweet: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "# Print a sample tweet\n",
        "print(\"Sample Tweet:\", all_tweets[0])\n",
        "print(\"Label:\", labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfc2a04a",
      "metadata": {
        "id": "bfc2a04a"
      },
      "source": [
        "### **Exercise 9: Preprocess Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d1e984",
      "metadata": {
        "id": "b4d1e984"
      },
      "source": [
        "Apply the custom preprocessing pipeline to the entire dataset of tweets. Store the result in ``preprocessed_tweets``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "39e8c02f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39e8c02f",
        "outputId": "72582eef-bb28-494c-bcb8-d4ed653dc854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer and stop words list\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define the custom preprocessing pipeline function\n",
        "def text_preprocessing_pipeline(text):\n",
        "    # Step 1: Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 2: Remove stop words\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    # Step 3: Remove punctuation (already handled with stopwords removal)\n",
        "    # Step 4: Apply lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "edeef254",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edeef254",
        "outputId": "3b7bc2f8-4d5e-4c97-84b8-936090a2a6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Tweets Sample: ['followfriday', 'france_inte', 'pkuchly57', 'milipol_paris', 'top', 'engaged', 'member', 'community', 'week']\n"
          ]
        }
      ],
      "source": [
        "# Apply the preprocessing pipeline to the entire dataset of tweets\n",
        "preprocessed_tweets = [text_preprocessing_pipeline(tweet) for tweet in all_tweets]\n",
        "\n",
        "# Print a sample preprocessed tweet\n",
        "print(\"Preprocessed Tweets Sample:\", preprocessed_tweets[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8658daf",
      "metadata": {
        "id": "a8658daf"
      },
      "source": [
        "### **Exercise 10: Feature Extraction on Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381f161c",
      "metadata": {
        "id": "381f161c"
      },
      "source": [
        "Extract features from the preprocessed tweets using **Bag of Words** and **TF-IDF**. Store the results in ``X_bow`` and ``X_tfidf``, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bee42f6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bee42f6c",
        "outputId": "3a349c25-52a0-4e6f-b75b-cc5f197e56e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Feature Matrix Shape: (10000, 19900)\n",
            "TF-IDF Feature Matrix Shape: (10000, 19900)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Step 1: Create a Bag of Words representation\n",
        "# Convert preprocessed tweets from list of tokens to string format for CountVectorizer\n",
        "preprocessed_tweets_str = [' '.join(tweet) for tweet in preprocessed_tweets]\n",
        "\n",
        "# Initialize CountVectorizer for Bag of Words\n",
        "bow_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the preprocessed tweets into a BoW representation\n",
        "X_bow = bow_vectorizer.fit_transform(preprocessed_tweets_str)\n",
        "\n",
        "# Step 2: Create a TF-IDF representation\n",
        "# Initialize TfidfVectorizer for TF-IDF representation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the preprocessed tweets into a TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_tweets_str)\n",
        "\n",
        "# Display the shapes of the results\n",
        "print(\"BoW Feature Matrix Shape:\", X_bow.shape)\n",
        "print(\"TF-IDF Feature Matrix Shape:\", X_tfidf.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4912687c",
      "metadata": {
        "id": "4912687c"
      },
      "source": [
        "## **7. Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948ed4ac",
      "metadata": {
        "id": "948ed4ac"
      },
      "source": [
        "In this lab, you explored a wide range of NLP techniques, from basic text preprocessing to advanced feature extraction and analysis. You also worked with a real-world dataset of tweets and applied your knowledge to preprocess and extract features for sentiment analysis.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}